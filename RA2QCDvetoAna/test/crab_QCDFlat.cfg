[CRAB]

jobtype = cmssw
#
#--- scheduler
# caf: CERN-CAF, condor: LPC-CAF, glite: GRID
#scheduler = glite
scheduler = condor
### NOTE: just setting the name of the server (pi, lnl etc etc )
###       crab will submit the jobs to the server...
#
#--- server_name
# comment-out: CERN-CAF, LPC-CAF, cern: GRID
#server_name = cern
#use_server = 0

[CMSSW]

lumi_mask=Cert_160404-172255_7TeV_PromptReco_Collisions11_JSON.txt
total_number_of_lumis = -1
lumis_per_job = 20

#show_prod=1

### The data you want to access (to be found on DBS)
dbs_url = http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
datasetpath = /QCD_Pt-15to3000_TuneZ2_Flat_7TeV_pythia6/Summer11-PU_S3_START42_V11-v2/GEN-SIM-RECO
use_parent = 0

#runselection = 165088-166502

#debug_wrapper=1

### The ParameterSet you want to use
###########################
pset = myRa2Ana_cfg.py

### Splitting parameters
#total_number_of_events = -1 
#number_of_jobs = 20
###########################

pycfg_params = GlobalTag=GR_R_42_V19::All hltName=HLT hltSelection=HLT_IsoMu17_v* mcInfo=0

### The output files (comma separated list)
###########################
output_file = ra2SUSYPAT.root
#get_edm_output = 1
#output_file = rereco_ECALRecovery.root
###########################

[USER]

### OUTPUT files Management
##  output back into UI
return_data = 0

### OUTPUT files INTO A SE
copy_data = 1

###email notifications
thresholdLevel = 100

#additional_input_files = muonIsoRecoEff*.root, effBtag.root

#storage_element        = cmssrm.fnal.gov
#storage_path           = /srm/managerv2?SFN=/resilient/bues90/
#user_remote_dir = /QCDFlat_Pt500to1000-madgraph_Spring10_v1.4/

#publish!!
storage_element = cmssrm.fnal.gov
storage_port = 8443
storage_path = /srm/managerv2?SFN=/11
user_remote_dir = /store/user/samantha/2011RA2/Aug11/
publish_data = 1
publish_data_name = _ra2StdCleaned_NoMHT
dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_01_writer/servlet/DBSServlet
check_user_remote_dir = 0

[GRID]

#remove_default_blacklist=1
#role=t1access

## RB/WMS management:
#rb = CERN
#proxy_server = myproxy.cern.ch

##  Black and White Lists management:
## By Storage
##se_black_list = T0,T1

#se_white_list = grid-srm.physik.rwth-aachen.de

## By ComputingElement
#ce_black_list = cmssrm.fnal.gov, cmsdca.fnal.gov
#ce_black_list = srm.ciemat.es,srm-3.t2.ucsd.edu,hephyse.oeaw.ac.at,maite.iihe.ac.be,t2-srm-02.lnl.infn.it,sbgse1.in2p3.fr,cmssrm.hep.wisc.edu,cmsdcache.pi.infn.it,srm.minnesota.edu,storm.ifca.es
#ce_white_list = storm-fe-cms.cr.cnaf.infn.it
#ce_white_list = srmcms.pic.es

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
#batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
#globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))
